# AGENT GUIDANCE & ARCHITECTURE

This repository contains a research harness for a Bachelorâ€™s Honours Thesis targeting **ITiCSE/SIGCSE**.
The goal is **not** to build a grading tool, but to measure the **Cognitive Alignment** of LLMs with CS Education theory.

---

## 1. Core Architecture

The system operates on a **Synthetic Injection -> Blind Detection -> Semantic Alignment** pipeline.

### Stage 1: The Notional Machine Injection (Dataset Generation)
*   **Source:** We do not use random bugs. We use rigorous JSON definitions of Notional Machines.
*   **Files:** `data/groundtruth/A1_ScalarState.json`, `data/groundtruth/A2_ControlFlow.json`, etc.
*   **Mechanism:**
    *   Select a specific misconception (e.g., `NM_STATE_REACTIVE`).
    *   Inject it into a valid solution using the `student_thinking` and `code_pattern` fields from the JSON.
    *   **Constraint:** One misconception per file to ensure clean labeling.

### Stage 2: The Blind Diagnosis (Detection)
*   **Philosophy:** We do *not* give the LLM the answer key. We want to see if it can "discover" the Notional Machine failure.
*   **The Instrument:** LLMs must output a strict JSON structure (defined via Pydantic) containing:
    1.  `inferred_category_name` (String, open-ended).
    2.  `student_thought_process` (Narrative description).
    3.  `evidence` (Line numbers).
*   **Prompts:** We run 4 distinct strategies:
    1.  **Baseline:** "Find the bug."
    2.  **Taxonomy:** "Classify using these definitions..."
    3.  **CoT:** "Trace the memory state line-by-line..."
    4.  **Socratic:** "Diagnose the student's mental model..."

### Stage 3: Semantic Alignment (Evaluation)
*   **The Challenge:** The LLM might call it "Auto-Update Error" while we call it "Reactive State Machine."
*   **The Solution:** We use **Cosine Similarity** (via `sentence-transformers` or OpenAI Embeddings).
*   **Logic:**
    *   Embed the `student_thought_process` generated by the LLM.
    *   Embed the `explanation` field from the Ground Truth JSON.
    *   If `Similarity > Threshold (e.g., 0.85)`, count as a **True Positive**.
*   **Why?** This allows us to quantify "Conceptual Understanding" rather than just keyword matching.

---

## 2. The Data Hierarchy (The "Complexity Gradient")

Agents must maintain separation between these distinct levels of abstraction to support **RQ1**.

| Assignment ID   | Focus                  | Notional Machine Theory                  | Complexity            |
| :-------------- | :--------------------- | :--------------------------------------- | :-------------------- |
| **A1**          | Variables, Math, Input | **Reactive State / Anthropomorphic I/O** | Scalar State (Low)    |
| **A2**          | Loops, Conditionals    | **Teleological Flow**                    | Temporal State (Med)  |
| **A3**          | Arrays, Strings        | **Spatial Adjacency**                    | Spatial State (High)  |
| **A4** (Future) | Objects, References    | **Heap / Indirection**                   | Reference State (Max) |

---

## 3. Methodology Standards

**1. The "Honest N-Count"**
*   Never report results based on the number of API calls.
*   Report results based on **Unique Misconception Instances**.
*   *Metric:* "Of the 20 files containing 'Spreadsheet View', how many did GPT-4 correctly diagnose?"

**2. The "Hallucination Trap"**
*   We must track **False Positives** aggressively.
*   If an LLM flags a "Syntax Error" on code that compiles, it is a critical failure.
*   If an LLM invents a "Logic Error" based on a misunderstanding of the prompt, it is a "Pedagogical Hallucination."

**3. Statistical Rigor**
*   Use Bootstrapping (Confidence Intervals) for all F1 scores.
*   When comparing Prompt Strategies (e.g., CoT vs. Socratic), use statistical significance tests (e.g., McNemar's test) because the test set is paired (same code, different prompts).

---

## 4. Operational Directives for Agents

1.  **Do not invent new taxonomies.** Stick to the IDs defined in `data/groundtruth/`.
2.  **Preserve the Pydantic Models.** The output schema is the scientific instrument. Do not change fields without updating the analysis script.
3.  **Focus on the "Gap".** When analyzing results, always look for the delta between **Syntax Detection** (likely 99%) and **State Detection** (likely <30%). That delta is the thesis.