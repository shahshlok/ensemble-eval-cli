# EduBench: AI Grading Benchmark

A comprehensive benchmarking system for comparing GPT-5 Nano and GPT-OSS 120B grading strategies on student code submissions. Features automatic result persistence via SQLite and a rich CLI interface.

---

## Quick Start

### Run a Benchmark

```bash
# Interactive mode
uv run bench benchmark

# Direct command
uv run bench benchmark --mode direct
```

### Access Your Results

```bash
# Launch analysis menu
uv run bench benchmark
# Select option [5] Analysis
# Select option [1] Restore JSON from Database
```

---

## Features

âœ… **Three Grading Strategies**
- **Direct Grading**: Grade student code directly against rubric
- **Reverse Grading**: Generate ideal solution first, then compare
- **Ensemble (EME)**: Multi-model ensemble approach from RIAYN paper

âœ… **Automatic Database Persistence**
- All benchmark results automatically stored in SQLite
- JSON files remain as temporary workspace for ad-hoc analysis
- Validated JSON in, validated JSON out

âœ… **Rich CLI Interface**
- Interactive menu system
- Real-time progress bars
- Beautiful result tables with Rich library
- Cross-strategy comparison views

âœ… **Data Validation**
- JSON Schema validation on all results
- Business rule checks (metrics calculations)
- Graceful error handling

---

## Project Structure (High-Level)

```text
EME_testing/
â”œâ”€â”€ cli.py                   # Main CLI application (bench command)
â”œâ”€â”€ single_submission.py     # One-off EME run for a single student
â”œâ”€â”€ modes/                   # Grading strategies (direct / reverse / EME)
â”œâ”€â”€ prompts/                 # Grading prompt templates
â”œâ”€â”€ utils/                   # Shared utilities (models, evaluation, validation, display)
â”œâ”€â”€ db/                      # SQLite schema + database manager
â”œâ”€â”€ docs/                    # Documentation hub (see below)
â”œâ”€â”€ data/                    # Temporary JSON workspace (results_*.json)
â”œâ”€â”€ student_submissions/     # Input code submissions (.java)
â”œâ”€â”€ evaluation_schema.json   # JSON Schema for validation
â””â”€â”€ evaluations.db           # SQLite database (auto-created)
```

For a **beginnerâ€‘friendly tour of the codebase**, diagrams, and detailed examples, see:

- `docs/INDEX.md` â€“ docs hub and quickstart
- `docs/ARCHITECTURE.md` â€“ pipeline diagrams and module map
- `docs/DATABASE.md` â€“ database schema and ingestion
- `docs/JSON_OUTPUT.md` â€“ structured JSON outputs from models

---

## Database System

### Overview

The database automatically captures all benchmark runs for persistence and historical analysis without changing the JSON-based workflow.

**Philosophy**: "Validated JSON in, validated JSON out"

### Schema

- **`runs` table**: Tracks each evaluation run (timestamp, filename, strategy)
- **`evaluations` table**: Stores individual student evaluations as JSON text

### Automatic Ingestion

Every benchmark run automatically:
1. Saves results to `data/results_{strategy}_{timestamp}.json`
2. Validates against `evaluation_schema.json`
3. Ingests into SQLite database (`evaluations.db`)

### Restoration

Restore all JSON files from database to `data/` directory:

```bash
uv run bench benchmark
# Select [5] Analysis â†’ [1] Restore JSON from Database
```

Or programmatically:

```python
from db.manager import restore_json_files
from pathlib import Path

files_restored = restore_json_files("evaluations.db", Path("data"))
```

### Documentation

ðŸ“š **[Complete Database Documentation](docs/DATABASE.md)**

Covers:
- Architecture and schema design
- API reference for `db/manager.py`
- Validation workflow
- Usage examples and recipes
- Error handling and troubleshooting
- Future enhancements

---

## CLI Commands

### Benchmark Command

```bash
# Interactive mode (recommended)
uv run bench benchmark

# With flags
uv run bench benchmark --mode direct
uv run bench benchmark --mode all      # Run all strategies
uv run bench benchmark --advanced      # Show per-student progress
```

**Available Modes**:
- `direct` - Direct grading against rubric
- `reverse` - Generate reference solution first
- `eme` - Ensemble method
- `all` - Run all three strategies and compare

### Analysis Menu

Access from the main menu (option 5) or when running benchmarks:

**Options**:
1. **Restore JSON from Database** - Export all evaluation results to `data/`
2. **Back to Main Menu** - Return to benchmark selection

---

## Validation

All evaluation results are validated before database storage:

### JSON Schema Validation
- Field types and required fields
- Nested structure validation
- Score ranges (0-100%)

### Business Rule Validation
- `avg_pct` must match calculated average
- `diff_pct` must match absolute difference
- Student identifiers must be non-empty

**Example Validation Output**:
```
âœ“ Validation passed

OR

âœ— Validation failed with 2 error(s)

Errors:
  1. Record 3 (student: Smith_John_123456):
     - avg_pct (57.5) doesn't match calculated average (58.00)
  2. Record 5: Missing required field 'metrics'
```

If validation fails, the file is **not ingested** (database remains consistent), but the JSON file is still saved to `data/`.

---

## Development

### Installation

```bash
# Clone repository
git clone <repo-url>
cd EME_testing

# Install dependencies
uv sync
```

### Project Dependencies

- `typer` - CLI framework
- `rich` - Terminal UI and formatting
- `jsonschema` - JSON validation
- `openai` / LLM clients - Model interactions
- `sqlite3` - Built-in Python (no install needed)

### Testing

```bash
# Test database ingestion
python -c "
from pathlib import Path
from db.manager import DatabaseManager

db = DatabaseManager()
db.init_db()

for f in Path('data').glob('results_*.json'):
    db.ingest_results_file(f)
    print(f'âœ“ Ingested {f.name}')
"

# Test restoration
python -c "
from pathlib import Path
from db.manager import restore_json_files

count = restore_json_files('evaluations.db', Path('data'))
print(f'âœ“ Restored {count} files')
"
```

---

## Configuration

### Environment Variables

Create a `.env` file:

```env
QUESTION="Write a Java program that computes the sum of integers from 1 to 100."

RUBRIC='{
  "correctness": {"weight": 40, "description": "Program produces correct output"},
  "compilation": {"weight": 20, "description": "Code compiles without errors"},
  "style": {"weight": 20, "description": "Code follows conventions"},
  "documentation": {"weight": 20, "description": "Adequate comments"}
}'

# API Keys
OPENAI_API_KEY=sk-...
OPENROUTER_API_KEY=sk-or-...
OPENROUTER_MODEL=google/gemini-2.5-flash-preview-09-2025
# Optional analytics headers for OpenRouter rankings (can be left unset)
# OPENROUTER_SITE_URL=https://your-app-url.com
# OPENROUTER_SITE_NAME=Your App Name
```

---

## Output Format

### Evaluation JSON Structure

Each evaluation file (`data/results_{strategy}_{timestamp}.json`) contains an array:

```json
[
  {
    "student": "Smith_John_123456",
    "gpt5_nano_result": {
      "total_score": 95,
      "max_possible_score": 100,
      "overall_feedback": "Excellent work..."
    },
    "gpt_oss_120b_result": {
      "total_score": 90,
      "max_possible_score": 100,
      "overall_feedback": "Good implementation..."
    },
    "metrics": {
      "gpt5_nano": {"total": 95.0, "max": 100.0, "pct": 95.0},
      "gpt_oss_120b": {"total": 90.0, "max": 100.0, "pct": 90.0},
      "avg_pct": 92.5,
      "diff_pct": 5.0,
      "flag": "âœ…",
      "comment": "Models agree within tolerance"
    }
  }
]
```

### Database Storage

- **Runs**: Metadata about each benchmark run
- **Evaluations**: Individual student results stored as JSON text
- **Indexes**: Optimized for querying by student or run

---

## Future Enhancements

Potential features (see [DATABASE.md](docs/DATABASE.md#future-enhancements) for details):

- Query interface for database (`uv run bench query --student "Smith_John"`)
- Statistics dashboard across all runs
- Export filters (by strategy, date range, student)
- Run comparison tool
- Annotation system for manual notes
- JSONB virtual columns for advanced queries

---

## Contributing

When adding features:

1. **Follow separation of concerns**:
   - CLI logic in `cli.py`
   - Database operations in `db/manager.py`
   - Grading strategies in `modes/`
   - Shared utilities in `utils/`

2. **Maintain validation**:
   - Update `evaluation_schema.json` for schema changes
   - Add business rules to `utils/validation.py`

3. **Document changes**:
   - Update relevant docs in `docs/`
   - Add docstrings to all functions
   - Include usage examples

4. **Test database operations**:
   - Verify round-trip integrity (ingest â†’ restore)
   - Test validation with invalid data
   - Check error handling

---

## License

[Your License Here]

## Contact

[Your Contact Info Here]
